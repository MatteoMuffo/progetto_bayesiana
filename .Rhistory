Rim_area               #beta19
)
# Hyperparameters:
mu0=rep(0, dim(X)[2])
c=50
S0=c*solve(t(X)%*%X)
# NO RANDOM
data=list( y=RNFL_average, X=X,  npat= length(unique(Patient)),
mu0=mu0, S0=S0,  numerosity = numerosity, kk=kk)      # dati che passo a jags
#fixed coefficients initialization:
beta= rep(0,dim(X)[2])
#inits = function() {list( beta=beta, b=b, invD=invD, sigma0=50,sigma1=50)}
inits = function() {list( beta=beta, sigma0=50,sigma1=50)}
modelRegress=jags.model("data5_norandom.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
update(modelRegress,n.iter=19000)
variable.names=c("beta", "sigma0","sigma1")
n.iter=50000
thin=10
library(coda)
library(plotrix)
outputRegress=coda.samples(model=modelRegress,variable.names=variable.names,n.iter=n.iter,thin=thin)
outputRegress_mcmc <- as.mcmc(outputRegress)
quartz()
plot(outputRegress_mcmc)
data.out=data.frame(data.out)
attach(data.out)
n.chain=dim(data.out)[1]
n.chain
#summary(data.out)
#head(data.out)
save.image("../R_object/model_6.RData")
quartz()
plot(outputRegress_mcmc)
quartz()
plot(outputRegress_mcmc)
quartz()
plot(outputRegress_mcmc)
quartz()
acfplot(outputRegress_mcmc)
rm(list=ls())
setwd("~/Documents/universita/bayesian_statistics-Guglielmi/Lessons material/BNP")
a=1 ##### TOTAL MASS parameter
M <- 1000 # truncation level   M=1000 o M=500
Y <- vector(length=M)    ## Y is the vector of the beta proportions in the stick-breaking
tau <-  vector(length=M) ## support points of the DP (theta_i at the blackboard)
V <- vector(length=M)    ## V is the vector of the weights of the support points tau_i
### SIMULATION
set.seed(13)  # fisso il seme aleatorio
Y <- rbeta(M,1,a) #simulated values of the M rvs which are beta(1,a)-distributed
tau <- rnorm(M,0,1) # simulated tau_i, iid from alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod   ## V_i = Y_i \prod_{j=1}^{i-1}(1-Y_j)
## "spezzo i bastoncini" utilizzando le M v.a. Y_i prima simulate
print(sum(V))  ## Compute the sum of all weights so far
V <- V/sum(V)  ## Rinormalization of the weights (needed because truncation of the infinite series)
x11()
curve(dnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,ylim=c(0,0.5),xlab=" ",ylab="",cex.axis=1.5) #mean parameter alpha_0 (in magenta)
abline(h=0,lty=2)
lines(tau,V,"h",lwd=3,col="red")
#title("Weights and support points for a simulated trajectory of a DP")
sort(V) # When a is SMALL, there is one SINGLE LARGE weight (about 1);
# all the other weights are almost 0, so that we do NOT see them in the plot.
# When a is LARGE, all the weights V_i are small, and their values are very similar
##### VISUALIZATION of the trajectory of the corresponding distribution function
oth <- order(tau)
#postscript("trajectories_a1.eps")
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,xlab=" ",ylab="",cex.axis=1.5) # in magenta la f.r. di alpha_0, la misura media
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col="red",lwd=3)
##### TWO more trajectories
set.seed(99)
Y <- rbeta(M,1,a)
tau <- rnorm(M,0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod
print(sum(V))
V <- V/sum(V)
oth <- order(tau)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col="green",lwd=3)
set.seed(76)
Y <- rbeta(M,1,a)
tau <- rnorm(M,0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod
print(sum(V))
V <- V/sum(V)
oth <- order(tau)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col="blue",lwd=3)
### SAMPLING MANY trajectories of the DP df: we add them to the same plot
gra=gray(1:100/100)
gra=rep(gra,10)
for(j in 1:200){
set.seed(j)
Y <- rbeta(M,1,a)
tau <- rnorm(M,0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod
print(sum(V))
V <- V/sum(V)
oth <- order(tau)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col=gra[j])
}
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,add=T)
x11()
a=0.5
M <- 750 # livello di troncamento   M=500
Y <- vector(length=M) ##Il vettore V conterr? le v.a. beta della costruzione stick breaking
tau <-  vector(length=M) ## i punti di supporto della mpa processo di Dirichlet - PRIMA li abbiamo chiamati theta
V <- vector(length=M)
# Simulo
Y <- rbeta(M,1,a) #simulo le beta
tau <- rnorm(M,0,1) # simulo i tau in modo iid da alpha_0= N(0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod ## "spezzo i bastoncini" utilizzando le Y prima sumulate
print(sum(V))  ##Valuto la somma dei V
V <- V/sum(V)  ## Rinormalizzo i V
#postscript("comparison.eps")
par(mfrow=c(1,3))
oth <- order(tau)
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,xlab="",ylab="",cex.axis=1.5)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col="red",lwd=3)
title("Total mass a=0.5")
gra=gray(1:100/100)
gra=rep(gra,10)
for(j in 1:50){   #for(j in 1:100){
set.seed(j)
Y <- rbeta(M,1,a)
tau <- rnorm(M,0,1)
cprod <- cumprod(1-Y)
cprod  <- c(1,cprod[1:M-1])
V <- Y*cprod
print(sum(V))
V <- V/sum(V)
oth <- order(tau)
lines(c(min(tau)-100,tau[oth],max(tau)+100),c(0,cumsum(V[oth]),1),type="s",col=gra[j])
}
curve(pnorm(x,0,1),from=-4,to=4,col="magenta",lwd=2,add=T)
rm(list=ls())
# DATI CENTRATI
# Importo i dati
input = read.table("db_centrato_xDP.txt",header=T)
# DATI CENTRATI
# Importo i dati
input = read.table("Database+for+GLMM_DP.txt",header=T)
names(input)
summary(input)
n.data=dim(input)[1] # numero totale pazienti
J=17 # numero ospedali
M=40 # troncamento serie Sethuraman
#######################################
library(rjags)      # interfaccia R con JAGS
library(plotrix)    # per fare plot CIs
set.seed(1)         # fisso il seed per poter riprodurre
#  genero una lista con i dati da passare a JAGS
data <- list(Y=input$vivo,
AGE=input$eta, KILLIP=input$killip,LOGOB = input$logOB,
CENTRO=input$centro,
n=n.data,J=J,M=M)
r=rep(0.5,M)
theta=rep(0,M)
S=rep(1,J)
inits = list(beta=rep(0,3),a = 1,
lambda.bb = 1,
r=r,
theta= theta,
S = S,
Snew = 1,
.RNG.seed = 2,
.RNG.name = 'base::Wichmann-Hill'
)
modelGLMM_DP=jags.model("GLMM_DP_model.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
modelGLMM_DP=jags.model("File+bug+for+GLMM_DP.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
# Faccio un update del modello per il numero di iterazioni specificato SENZA salvare nulla
update(modelGLMM_DP,19000)
variable.names=c("bb", "beta",  "tau.bb", "newcentro", "alpha","K")
# Monitoro i parametri:
n.iter=50000
thin=10
outGLMM_DP=coda.samples(model=modelGLMM_DP,variable.names=variable.names,n.iter=n.iter,thin=thin)
# salvo l'intera catena dei parametri monitorati (si tratta di una lista mcmc)
save(outGLMM_DP,file='GLMMDP_output.Rdata')
# Importo i dati
input = read.table("db_centrato_xDP.txt",header=T)
names(input)
n.data=dim(input)[1] # numero totale pazienti
J=17 # numero ospedali
M=40 # troncamento serie Sethuraman
library(coda)        # pacchetto per analizzare catene
library(plotrix)     # per fare plot CIs
data=as.matrix(outGLMM_DP) # trasformo il dataframe in matrice
data=data.frame(data)
attach(data)
n.chain=dim(data)[1]   # lunghezza catena (final sample size)
par(mfrow=c(1,3))
plot(data[,'beta.1.'],main='et?',type='l')
quartz()
par(mfrow=c(1,3))
plot(data[,'beta.1.'],main='et?',type='l')
plot(data[,'beta.2.'],main='log(OB)',type='l')
plot(data[,'beta.3.'],main='killip',type='l')
quartz()
par(mfrow=c(1,3))
plot(density(data[,'beta.1.']),main='et?')
plot(density(data[,'beta.2.']),main='log(OB)')
plot(density(data[,'beta.3.']),main='killip')
# et? e killip sono significative; un po' meno significativo ? log(OB)
mean(data[,'beta.1.']<0)
mean(data[,'beta.3.']<0)
mean(data[,'beta.2.']<0)
x11()
par(mfrow=c(3,3)) #qualche traceplot degli effetti casuali
plot(data[,'bb.1.'],main='b1',type='l')
plot(data[,'bb.2.'],main='b2',type='l')
plot(data[,'bb.3.'],main='b3',type='l')
plot(data[,'bb.7.'],main='b7',type='l')
plot(data[,'bb.10.'],main='b10',type='l')
plot(data[,'bb.13.'],main='b13',type='l')
plot(data[,'bb.15.'],main='b15',type='l')
plot(data[,'bb.16.'],main='b16',type='l')
plot(data[,'bb.17.'],main='b17',type='l')
x11()
par(mfrow=c(1,2))
plot(data[,'alpha'],main='massa totale',type='l') #se ho assegnato una prior per alpha
plot(data[,'K'],main='K_n',type='l')
x11()
par(mfrow=c(1,2))
plot(density(data[,'alpha']),main='massa totale')
plot(table(data[,'K']),main='K_n')
mean(alpha);var(alpha)
mean(data[,'K']);var(data[,'K'])
par(mfrow=c(1,3))
plot(data[,'tau.bb'],main='tau',type='l')
plot(1/sqrt(data[,'tau.bb']),main='lambda',type='l')
plot(density(1/sqrt(data[,'tau.bb'])),main='lambda')
quartz()
par(mfrow=c(1,3))
plot(data[,'tau.bb'],main='tau',type='l')
plot(1/sqrt(data[,'tau.bb']),main='lambda',type='l')
plot(density(1/sqrt(data[,'tau.bb'])),main='lambda')
patients=rep(0,J)
for(i in 1:J){
patients[i]=length(which(input$centro==i))
}
sort_patients=sort(patients,index.return=T)   # ordino
# ricalcolo i quantili per ogni ospedale in ordine di numero di pazienti
Q=matrix(nrow=J+1, ncol=3)
for (j in 1:J){
Q[j,]=quantile(data[, 2 + sort_patients$ix[j]  ],probs=c(0.025,0.5,0.975))
}
Q[J+1,]=quantile(data$newcentro,probs=c(0.025,0.5,0.975))
colnames(Q) <- c("2.5","median","97.5")
# symbol for point estimate: all points are round,
#    the last point is x (new random hospital)
pch=c(rep(21,J),4)
plotCI(x=seq(1,J+1),y=Q[,2],uiw=(Q[,3]-Q[,2]) ,liw=(Q[,2]-Q[,1]),pch=pch,
scol=1 , xlab="hospitals (sort by increasing number of patients)",
ylab="b_j", main="CIs of the Random Intercept")
x11()
plotCI(x=seq(1,J+1),y=Q[,2],uiw=(Q[,3]-Q[,2]) ,liw=(Q[,2]-Q[,1]),pch=pch,
scol=1 , xlab="hospitals (sort by increasing number of patients)",
ylab="b_j", main="CIs of the Random Intercept")
abline(h=mean(Q[,2]))
J=17
label.mat = as.matrix(data[,3:19]) # extract cluster labels
m=J
G=n.chain
pihat <- matrix(0,ncol=m,nrow=m)
for(i in 1:G){
ss <- label.mat[i,]
cij <- outer(ss,ss,'==')
pihat <- pihat+cij
}
pihat <- pihat/G
#####Binder loss function
FF <- vector("numeric")
K <- 0.7 #prova con K=0.5, >=0.7
for(i in 1:G){
ss <- label.mat[i,]
cij <- outer(ss,ss,'==')
pluto <- (pihat-K)*as.matrix(cij)
pluto <-  pluto[upper.tri(pluto)]
FF[i] <- sum(pluto)
}
plot(FF)
ind.bind <- which.max(FF)[1]
label.mat[ind.bind,]#leti(ind.bind,L,G,m)
#plot(FF)
ll.bind <- label.mat[ind.bind,] #leti(ind.bind,L,G,m)
unici <- unique(ll.bind)
unici
l.uni <- length(unici)# numero di gruppi stimato
l.uni
ncl=l.uni
for(i in 1:ncl){
print(as.numeric(which(ll.bind==unici[i])))
}
##########################################################################?
### Disegno le intercette casuali al primo livello con colori diversi a seconda del gruppo
Sest=rep(0,J)
for(i in 1:ncl){
Sest[as.numeric(which(ll.bind==unici[i]))] = i
}
Sest
table(Sest)
nclusLG=length(unique(Sest))
nclusLG
bins = as.numeric(names(table(Sest)))
freqs = as.vector(table(Sest))
J=17
label=rep(0,J)
mylist=list()
for(i in 1:nclusLG)
{
in_gr=which(Sest==bins[i])
label[in_gr]=i
mylist[[i]]=in_gr
}
label
mylist
gr1 <- mylist[[1]]
gr2 <- mylist[[2]]
gr3 <- mylist[[3]]
gr4 <- mylist[[4]]
gr5 <- mylist[[5]]
colore = rep('ciao',J)
colore[gr1]='magenta'
colore[gr2]='red'
colore[gr3]='green'
colore[gr4]='blue'
colore[gr4]='siena'
pch=rep(20,J)
# Nella matrice 'data' dalla colonna 3 in poi ci sono i b,
# gli effetti casuali (hospital-specific) al primo livello
# Per verificare il contenuto delle colonne di 'data': names(data)
# ricalcolo i quantili per ogni ospedale in ordine di numero di pazienti
Q=matrix(nrow=J, ncol=3) #matrix(nrow=J, ncol=3)
for (j in 1:J){
Q[j,]=quantile(data[, 2 + j ],probs=c(0.025,0.5,0.975))
}
plotCI(x=seq(1,J),y=Q[,2],uiw=(Q[,3]-Q[,2]) ,liw=(Q[,2]-Q[,1]),pch=pch,,xaxt='n',
col=colore,scol=colore,xlab="hospital",ylab=' ', main=" ", lwd=2)
colore
colore = rep('ciao',J)
colore[gr1]='magenta'
colore[gr2]='red'
colore[gr3]='green'
colore[gr4]='blue'
colore[gr4]='yellow'
pch=rep(20,J)
# Nella matrice 'data' dalla colonna 3 in poi ci sono i b,
# gli effetti casuali (hospital-specific) al primo livello
# Per verificare il contenuto delle colonne di 'data': names(data)
# ricalcolo i quantili per ogni ospedale in ordine di numero di pazienti
Q=matrix(nrow=J, ncol=3) #matrix(nrow=J, ncol=3)
for (j in 1:J){
Q[j,]=quantile(data[, 2 + j ],probs=c(0.025,0.5,0.975))
}
x11()
plotCI(x=seq(1,J),y=Q[,2],uiw=(Q[,3]-Q[,2]) ,liw=(Q[,2]-Q[,1]),pch=pch,xaxt='n',
col=colore,scol=colore,xlab="hospital",ylab=' ', main=" ", lwd=2)
axis(side=1,at=seq(1,J),labels=seq(1,J),cex.axis=0.8)
colore = rep('black',J)
colore[gr1]='magenta'
colore[gr2]='red'
colore[gr3]='green'
colore[gr4]='blue'
colore[gr4]='yellow'
pch=rep(20,J)
# Nella matrice 'data' dalla colonna 3 in poi ci sono i b,
# gli effetti casuali (hospital-specific) al primo livello
# Per verificare il contenuto delle colonne di 'data': names(data)
# ricalcolo i quantili per ogni ospedale in ordine di numero di pazienti
Q=matrix(nrow=J, ncol=3) #matrix(nrow=J, ncol=3)
for (j in 1:J){
Q[j,]=quantile(data[, 2 + j ],probs=c(0.025,0.5,0.975))
}
x11()
plotCI(x=seq(1,J),y=Q[,2],uiw=(Q[,3]-Q[,2]) ,liw=(Q[,2]-Q[,1]),pch=pch,xaxt='n',
col=colore,scol=colore,xlab="hospital",ylab=' ', main=" ", lwd=2)
axis(side=1,at=seq(1,J),labels=seq(1,J),cex.axis=0.8)
#media di tutte le mediane
media <- mean(Q[,2])
abline(h= media)
x11()
plotCI(x=seq(1,J),y=Q[,2],uiw=(Q[,3]-Q[,2]) ,liw=(Q[,2]-Q[,1]),pch=pch,xaxt='n',
col=colore,scol=colore,xlab="hospital",ylab=' ', main=" ", lwd=2)
axis(side=1,at=seq(1,J),labels=seq(1,J),cex.axis=0.8)
#media di tutte le mediane
media <- mean(Q[,2])
abline(h= media)
setwd("~/Documents/universita/bayesian_statistics-Guglielmi/GLAUCOMA_PROJECT/src")
rm(list=ls())
load("../R_object/Glaucoma_better_data.RData")
attach(mydata)
################JAGGAMENTO#######################
library(rjags)
# Compute useful values
numerosity<-as.integer(table(Patient))
kk=rep(0,length(unique(Patient)))
kk[1]=0
for (i in 2:length(unique(Patient))){
kk[i]=kk[i-1]+numerosity[i-1]
}
#covariates with fixed coefficents
#covariates with fixed coefficents
X=cbind(rep(1,length(Patient)),  #beta1 (intercept)
Black,               #beta2
Hispanic,            #beta3
#White,               # --> White categoria di riferimento
familiarity_yes,     #beta4
Sex,                 #beta5
POAG,                #beta6
Hypertension,        #beta7
HyperLipidemia,      #beta8
Cardiovascular_Dz,   #beta9
age65,               #beta10
prostaglandin,       #beta11
brimonidine,         #beta12
timolol,             #beta13
IOP,                     #beta14
MD,                      #beta15
macular_volume,          #beta16
Vert_integrated_rim_area__vol_,     #beta17
Horz_integrated_rim_width__area_,   #beta18
visit2,
Rim_area               #beta19
)
# Hyperparameters:
mu0=rep(0, dim(X)[2])
c=50
S0=c*solve(t(X)%*%X)
# NO RANDOM
data=list( y=RNFL_average, X=X,  npat= length(unique(Patient)),
mu0=mu0, S0=S0,  numerosity = numerosity, kk=kk)      # dati che passo a jags
#fixed coefficients initialization:
beta= rep(0,dim(X)[2])
#inits = function() {list( beta=beta, b=b, invD=invD, sigma0=50,sigma1=50)}
inits = function() {list( beta=beta, sigma0=50,sigma1=50)}
modelRegress=jags.model("data5_norandom.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
update(modelRegress,n.iter=19000)
variable.names=c("beta", "sigma0","sigma1")
n.iter=50000
thin=10
library(coda)
library(plotrix)
outputRegress=coda.samples(model=modelRegress,variable.names=variable.names,n.iter=n.iter,thin=thin)
data.out=as.matrix(outputRegress)
data.out=data.frame(data.out)
attach(data.out)
n.chain=dim(data.out)[1]
n.chain
#
library(reshape)
library(ggmcmc)
names(data.out)
beta=data.out[,grep("beta", names(data.out), fixed=TRUE)]
names(beta)
colnames(beta)=names(data.frame(X))
colnames(beta)[1]="Intercept"
names(beta)
tmp=melt(beta)
head(tmp)
colnames(tmp) = c("Parameter", "value")
CI.beta = apply(beta, 2, quantile, c(0.05, 0.95))  # 90%
CI.beta
p=ggs_caterpillar(tmp, thick_ci = c(0.05, 0.95), thin_ci = c(0.025, 0.975))
p + geom_vline(xintercept=0, col="orange")   #if you don't want colors, use this
p=ggs_caterpillar(tmp, thick_ci = c(0.05, 0.95), thin_ci = c(0.025, 0.975))
p + geom_vline(xintercept=0, col="orange")   #if you don't want colors, use this
p + geom_vline(xintercept=0, col="orange")   #if you don't want colors, use this
p
names(data.out)
beta=data.out[,grep("beta", names(data.out), fixed=TRUE)]
names(beta)
colnames(beta)=names(data.frame(X))
colnames(beta)[1]="Intercept"
names(beta)
tmp=melt(beta)
head(tmp)
colnames(tmp) = c("Parameter", "value")
CI.beta = apply(beta, 2, quantile, c(0.05, 0.95))  # 90%
CI.beta
p=ggs_caterpillar(tmp, thick_ci = c(0.05, 0.95), thin_ci = c(0.025, 0.975))
p + geom_vline(xintercept=0, col="orange")   #if you don't want colors, use this
library(reshape)
library(ggmcmc)
names(data.out)
beta=data.out[,grep("beta", names(data.out), fixed=TRUE)]
names(beta)
colnames(beta)=names(data.frame(X))
colnames(beta)[1]="Intercept"
names(beta)
tmp=melt(beta)
head(tmp)
colnames(tmp) = c("Parameter", "value")
CI.beta = apply(beta, 2, quantile, c(0.05, 0.95))  # 90%
CI.beta
p=ggs_caterpillar(tmp, thick_ci = c(0.05, 0.95), thin_ci = c(0.025, 0.975))
p + geom_vline(xintercept=0, col="orange")   #if you don't want colors, use this
p=ggs_caterpillar(tmp, thick_ci = c(0.05, 0.95), thin_ci = c(0.025, 0.975))
p + geom_vline(xintercept=0, col="orange")   #if you don't want colors, use this
# without intercept
p=ggs_caterpillar(tmp[which(tmp$Parameter!="Intercept"),], thick_ci = c(0.05, 0.95), thin_ci = c(0.025, 0.975))
p + geom_vline(xintercept=0, col="orange")
graphics.off()
# without intercept
p=ggs_caterpillar(tmp[which(tmp$Parameter!="Intercept"),], thick_ci = c(0.05, 0.95), thin_ci = c(0.025, 0.975))
p + geom_vline(xintercept=0, col="orange")
